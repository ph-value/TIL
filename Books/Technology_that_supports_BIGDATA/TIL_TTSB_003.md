
# 빅데이터를 지탱하는 기술

### 일자
2023.00.00.토

### CHAPTER 3 빅데이터의 분산 처리

### 책에서 기억하고 싶은 내용
#### 대규모 분산 처리의 프레임워크
- 구조화 데이터와 비구조화 데이터
  - 분산 스토리지에 수집된 데이터는 명확한 스키마를 갖지 않는 것도 많으므로 그냥 그대로는 SQL로 집계할 수 없다. 따라서, 먼저 필요한 것은 스키마를 명확하게 한 테이블 형식의 ‘구조화 데이터’로 변환하는 것이다.
  - 구조화 데이터 중 시간에 따라 증가하는 데이터를 팩트 테이블, 그에 따른 부속 데이터를 디멘전 테이블로 취급한다.
  - `Apache ORC`
    -  구조화 데이터를 위한 열 지향 스토리지로 처음에 스키마를 정한 후 데이터를 저장한다
  - `Apache Parquet`
    - 스키마리스에 가까운 데이터 구조로 되어 있어 JSON 같은 뒤얽힌 데이터도 그대로 저장할 수 있다.
  - 비구조화 데이터를 읽어 들여 열 지향 스토리지로 변환하는 과정에서는 데이터의 가공 및 압축을 위해 많은 컴퓨터 리소스가 소비된다. -> Hadoop과 Spark 등의 분산 처리 프레임워크 사용

- 분산 시스템
  - Hadoop의 기본 구성 요소
    - **분산 파일 시스템**(distributed file system)
      - HDFS(Hadoop Distribured File System)
    - **리소스 관리자**(resource manager)
      - YARN(Yet Another Resource Negotiator)
    - **분산 데이터 처리**(distributed data processing)
      - MapReduce
      - 한편, SQL 등의 쿼리 언어에 의한 데이터 집계가 목적이라면 그것을 위해 설계된 쿼리 엔진을 사용한다. 
        - `Apache Hive`: 쿼리를 자동으로 MapReduce 프로그램으로 변환하는 소프트웨어

#### 쿼리엔진
- Hive를 비롯한 대부분의 SQL-on-Hadoop의 쿼리 엔진은 MPP 데이터베이스처럼 데이 터를 내부로 가져오지 않아도 텍스트 파일을 그대로 집계할 수 있다.
  - CSV 파일을 그대로 집계하는 것은 비효율적이다. 쿼리를 실행시킬 때마다 매번 텍스트를 읽어 들이기 때문에 확실히 빠르다고는 말할 수 없다.
- 데이터의 구조화 -> 데이터 마트의 구축
  - ‘비정규화 테이블’을 만들기 위해 대화형 쿼리 엔진(Presto), 배치형 쿼리 엔진(Hive) 중 어떤 쿼리 엔진을 사용할 것인지에 따라 생각이 달라진다.💬
> Hive 쿼리는 SQL과 매우 유사하지만, 그 특성은 일반적인 RDB와는 전혀 다르다.
> Hive는 데이터베이스가 아닌 데이터 처리를 위한 배치 처리 구조다. 따라서, 읽어 들이는 데이터의 양을 의식하면서 쿼리를 작성하지 않으면 생각한 만큼의 성능이 나오지 않아 고민하게 된다.(105p/E133p)
  - 가능한 한 의식을 해서 ‘**초기에 팩트 테이블을 작게 하는 것**’이 빅데이터의 집계에서 중요하다
  - 최종적으로 GROUP BY로 데이터를 집계하고 싶다면, 테이블을 결합하기 전에 서브 쿼리 안에서 집계해 둘 수도 있다. 데이터의 양을 감소시킨 후에 테이블을 결합하는 것이 쿼리의 실행 시간을 단축할 수 있다.
- 데이터의 편차(data skew, 데이터 스큐)💬

#### Presto
> Presto는 SQL의 실행에 특화된 시스템으로, 쿼리를 분석하여 최적의 실행 계획을 생성 하고, 그것을 자바의 바이트 코드로 변환한다. 바이트 코드는 Presto의 워커 노드에 배포되고, 그것은 런타임 시스템에 의해 기계 코드로 컴파일된다.

> Presto는 대화식 쿼리의 실행에 특화되어 있기 때문에 텍스트 처리가 중심이 되는 ETL 프로세스 및 데이터 구조화에는 적합하지 않다.
- 분산 결합, 브로드캐스트 결합

#### Spark
> Hive에 의한 데이터의 구조화와 Presto에 의한 SQL의 실행에 관해 설명하 였는데, Spark로는 이 둘을 하나의 스크립트 안에서 실행할 수 있다. 즉, 텍스트 데이 터를 읽어 들여 열 지향 스토리지로 변환하고 그것을 또한 SQL로 집계해서 결과를 내보내는 등 일련의 프로세스를 한 번의 데이터 처리로 기술할 수 있다.

> 데이터 처리를 일종의 프로그래밍으로 생각하고 그것을 위한 실행 환경을 원한다면, Spark는 하나의 선택이 될 수 있다. 한편, SQL을 사용하고 싶은 것뿐이라면 처음부터 SQL에 특화한 쿼리 엔진과 MPP 데이터베이스를 사용하는 것이 좋다.

#### 데이터 마트의 구축
- 팩트 테이블_ 시계열 데이터 축적
  - 추가: 새로 도착한 데이터만을 증분으로 추가한다.
    - 추가에 실패한 것을 알아채지 못하면 팩트 테이블의 일부에 결손이 발생한다.
    - 추가를 잘못해서 여러 번 실행하면 팩트 테이블의 일부가 중복된다.
    - 나중에 팩트 테이블을 다시 만들고 싶은 경우의 관리가 복잡해진다.
      - 이런 문제 가능성을 줄이기 위해 **테이블 파티셔닝(table partitioning)** 기술 사용 
  - 치환: 과거의 데이터를 포함하여 테이블 전체를 치환한다.
> 하나의 데이터 처리에 어느 정도의 시간이 걸리는지는 빅데이터의 파이프라인을 고려한 다음에야 하나의 지표가 된다. 기준은 각 데이터 처리가 1시간 이내에 완료하도록 워크플로를 짜는 것이다. 1시간 이내에 팩트 테이블을 만들 수 있다면, 매번 치환하는 것으로 충분하다. 그것이 어려운 경우에만 추가를 이용한 워크플로를 고려하도록 한다.(121p/E140p)

- 집계 테이블_ 레코드 수 줄이기
  - `카디널리티 (cardinality)`: 칼럼이 취하는 값의 범위
    - EX)‘성별’과 같이 취할 수있는 값이 적은 것은 카디널리티가 작고, ‘IP 주소’와 같이 여러 값이 있는 것은 카디널리티가 커진다.
    - 집계 테이블을 작게 하려면 모든 칼럼의 카디널리티을 줄여야 한다.

- 스냅샷 테이블_ 마스터의 상태를 기록하기
  - 시간이 지남에 따라 점점 커지므로 이것도 일종의 팩트 테이블로 간주한다.
  - 다른 팩트 테이블과 결합함으로써 디멘전 테이블로도 사용할 수 있다.

- 이력 테이블_ 마스터 변화 기록하기
  - 정기적으로 모든 데이터를 스냅샷 하는 것이 아니라 변경된 데이터만을 증분으로 스냅샷 하거나 변경이 있을 때마다 그 내용을 기록하는 테이블

- [마지막 단계] 디멘전을 추가하여 비정규화 테이블 완성시키기
  > 세션 ID는 그냥 그대로는 카디날리티가 매우 커서 테이블을 집약해도 작아지지 않을 뿐만 아니라 시각화하는 것도 잘 안 된다. 그러므로 좀 더 카디널리티가 작은 디멘전을 만들어 결합하고, 시각화에 필요하지 않은 칼럼은 가급적 제거한다. 그에 따라 시각화 하기 쉽고 데이터양이 적은 비정규화 테이블이 완성된다. (시각화에 적합한 디멘전만 남기고 집계한다) (128p/E156p)


### 읽은 내용에 대한 내 생각



### 💬 기타 메모 (TBU)
#### 대화형 쿼리 엔진, 배치형 쿼리 엔진 서비스 정리
#### 데이터의 편차(data skew, 데이터 스큐) 예시 조사